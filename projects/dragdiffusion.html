<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<link href="https://fonts.googleapis.com/css2?family=Open+Sans&display=swap"
      rel="stylesheet">
<link rel="stylesheet" type="text/css" href="./drag_diffusion_resources/style.css" media="screen"/>

<html lang="en">
<head>
  	<title>DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing</title>
      <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/
          if you update and want to force Facebook to re-scrape. -->
  	<meta property="og:image" content="Path to my teaser.jpg"/>
  	<meta property="og:title" content="DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing" />
  	<meta property="og:description" content="Paper description." />
    <!-- Twitter automatically scrapes this. Go to https://cards-dev.twitter.com/validator?
        if you update and want to force Twitter to re-scrape. -->
    <meta property="twitter:card"          content="summary" />
    <meta property="twitter:title"         content="DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing" />
    <meta property="twitter:description"   content="" />
    <meta property="twitter:image"         content="Path to my teaser.jpg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Add your Google Analytics tag here -->
    <script async
            src="https://www.googletagmanager.com/gtag/js?id=UA-97476543-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());
        gtag('config', 'UA-97476543-1');
    </script>

</head>

<body>
<div class="container">
    <div class="title">
        DragDiffusion: Harnessing Diffusion Models
        <br>
        for Interactive Point-based Image Editing
    </div>

    <!-- <div class="venue">
        In Conference 20XX
    </div> -->

    <br><br>
    <div class="author">
        <a href="https://yujun-shi.github.io/">Yujun Shi</a><sup>1</sup>
    </div>
    <div class="author">
        Chuhui Xue<sup>2</sup>
    </div>
    <div class="author">
        Jiachun Pan<sup>1</sup>
    </div>
    <div class="author">
        Wenqing Zhang<sup>2</sup>
    </div>
    <div class="author">
        <a href="https://vyftan.github.io/">Vincent Y. F. Tan</a><sup>1</sup>
    </div>
    <div class="author">
        <a href="https://songbai.site/">Song Bai</a><sup>2</sup>
    </div>

    <br><br>

    <div class="affiliation"><sup>1&nbsp;</sup>National University of Singapore</div>
    <div class="affiliation"><sup>2&nbsp;</sup>ByteDance Inc.</div>

    <br><br>

    <div class="links"><a href="https://arxiv.org/abs/2306.14435">[Paper]</a></div>
    <div class="links">[Code] (Soon)</div>

    <br><br>

    <div class="video-with-text">
        <video width="1000" height="563" controls>
            <source src="./drag_diffusion_resources/clip_merge3x_compressed.mp4" type="video/mp4">
        </video>
    </div>

    <br><br><br>
    <p style="width: 80%;text-align: center">
        Our DragDiffusion enables "drag" editing for diffusion models.
        Given an input image, the user clicks handle points (<font color="red">red</font>),
        target points (<font color="blue">blue</font>) and
        draw a mask specifying editable region
        (<font color="gray">brighter areas</font>).
    </p>
    <br><br>
    <hr>


    <h1>Abstract</h1>
    <p style="width: 80%;text-align: justify">
        Precise and controllable image editing is a challenging task that has attracted significant attention.
        Recently, DragGAN enables an interactive point-based image editing framework and achieves impressive
        editing results with pixel-level precision.
        However, since this method is based on generative adversarial networks (GAN),
        its generality is upper-bounded by the capacity of the pre-trained GAN models.
        In this work, we extend such an editing framework to diffusion models and
        propose DragDiffusion. By leveraging large-scale pretrained diffusion models,
        we greatly improve the applicability of interactive point-based editing in real world scenarios.
        While most existing diffusion-based image editing methods work on text embeddings,
        DragDiffusion optimizes the diffusion latent to achieve precise spatial control.
        Although diffusion models generate images in an iterative manner,
        we empirically show that optimizing diffusion latent at one single step suffices to
        generate coherent results, enabling DragDiffusion to complete high-quality editing efficiently.
        Extensive experiments across a wide range of challenging cases
        (e.g., multi-objects, diverse object categories, various styles, etc.)
        demonstrate the versatility and generality of DragDiffusion.
    </p>

    <br><br>
    <hr>

    <h1>Method Overview</h1>
    <img style="width: 80%;" src="./drag_diffusion_resources/method_v3.jpg"
         alt="Method overview figure"/>
    <br>
    <p style="width: 80%;text-align: justify">
        Overview of DragDiffusion.
        As shown in part (A), we first finetune a LoRA on parameters of the UNet to reconstruct
        the user input image before editing.
        Next, part (B) illustrates the detailed editing process.
        Specifically, we first apply a DDIM inversion on the input image
        and obtain the latent at a certain time step.
        Then, based on the user editing instructions (i.e., handle points, target points, and mask),
        we optimize the inverted latent to supervise handle points to move to target points.
        Finally, DDIM denoising is applied on the optimized latent to obtain
        the final editing result.
    </p>

    <br><br>

    <hr>
    <h1>Editing Trajectories</h1>
    <div class="img-with-text">
        <img width="240" height="240" src="./drag_diffusion_resources/user_edit_oilpaint.png"/>
        <figcaption style="font-size:25px">user edit</figcaption>
    </div>
    <div class="img-with-text">
        <img width="240" height="240" src="./drag_diffusion_resources/user_edit_oilpaint_mountain.png"/>
        <figcaption style="font-size:25px">user edit</figcaption>
    </div>
    <div class="img-with-text">
        <img width="240" height="240" src="./drag_diffusion_resources/user_edit_cat_dog.png"/>
        <figcaption style="font-size:25px">user edit</figcaption>
    </div>
    <div class="img-with-text">
        <img width="240" height="240" src="./drag_diffusion_resources/user_edit_sculpture.png"/>
        <figcaption style="font-size:25px">user edit</figcaption>
    </div>

    <div class="video-with-text">
        <video width="240" height="240" controls>
            <source src="./drag_diffusion_resources/oilpaint_shorter.mp4" type="video/mp4">
        </video>
    <figcaption style="font-size:25px">editing trajectory</figcaption>
    </div>
    <div class="video-with-text">
        <video width="240" height="240" controls>
            <source src="./drag_diffusion_resources/oilpaint_mountain.mp4" type="video/mp4">
        </video>
    <figcaption style="font-size:25px">editing trajectory</figcaption>
    </div>
    <div class="video-with-text">
        <video width="240" height="240" controls>
            <source src="./drag_diffusion_resources/cat_dog.mp4" type="video/mp4">
        </video>
    <figcaption style="font-size:25px">editing trajectory</figcaption>
    </div>
    <div class="video-with-text">
        <video width="240" height="240" controls>
            <source src="./drag_diffusion_resources/sculpture.mp4" type="video/mp4">
        </video>
    <figcaption style="font-size:25px">editing trajectory</figcaption>
    </div>
    <br><br><br>
    <hr>

    <h1>Results</h1>
    <h2>General objects</h2>
    <img style="width: 80%;" src="./drag_diffusion_resources/general_object.jpg"
         alt="Results figure"/>

    <h2>Arts</h2>
    <img style="width: 80%;" src="./drag_diffusion_resources/art.jpg"
         alt="Results figure"/>

    <h2>Animals</h2>
    <img style="width: 80%;" src="./drag_diffusion_resources/animal.jpg"
         alt="Results figure"/>

    <h2>Scenes</h2>
    <img style="width: 80%;" src="./drag_diffusion_resources/scene.jpg"
         alt="Results figure"/>


    <br><br>
    <hr>

    <h1>Paper</h1>
    <div class="paper-thumbnail">
        <a href="https://arxiv.org/abs/2306.14435">
            <img class="layered-paper-big" width="100%" src="./drag_diffusion_resources/paper.jpg" alt="Paper thumbnail"/>
        </a>
    </div>
    <div class="paper-info">
        <h3>DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing</h3>
        <p>Yujun Shi, Chuhui Xue, Jiachun Pan, Wenqing Zhang, Vincent Y. F. Tan, Song Bai</p>
        <p>arXiv, 2023.</p>
        <pre><code>@InProceedings{shi2023dragdiffusion,
            title = {DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing},
            author = {Yujun Shi, Chuhui Xue, Jiachun Pan, Wenqing Zhang, Vincent Y. F. Tan, Song Bai},
            booktitle = {arXiv preprint arXiv:2306.14435},
            year = {2023},
        }
        </code></pre>
    </div>

    <br><br>
    <hr>

    <h1>Acknowledgements</h1>
    <p style="width: 80%;">
        This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful project</a>, and inherits the modifications made by <a href="https://github.com/jasonyzhang/webpage-template">Jason Zhang</a> and <a href="https://github.com/elliottwu/webpage-template">Elliott Wu</a>.
    </p>

    <br><br>
</div>

</body>

</html>
